{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHtml(querry, page):\n",
    "    \n",
    "    headers = {\n",
    "        'authority': 'www.ebay.com',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'accept-language': 'en-GB,en-US;q=0.9,en;q=0.8,hi;q=0.7,fr;q=0.6,gu;q=0.5,de;q=0.4',\n",
    "        'dnt': '1',\n",
    "        'referer': 'https://www.ebay.com/sch/i.html?_from=R40&_trksid=p4432023.m570.l1313&_nkw=mobile&_sacat=0',\n",
    "        'sec-ch-ua': '\"Not A(Brand\";v=\"99\", \"Google Chrome\";v=\"121\", \"Chromium\";v=\"121\"',\n",
    "        'sec-ch-ua-full-version': '\"121.0.6167.161\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-model': '\"\"',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "        'sec-ch-ua-platform-version': '\"15.0.0\"',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        '_from': 'R40',\n",
    "        '_nkw': querry,\n",
    "        '_sacat': '0',\n",
    "        '_pgn': page,\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://www.ebay.com/sch/i.html', params=params, headers=headers)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpageData(querry,page):\n",
    "    ItemList = []\n",
    "    response = getHtml(querry, page)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    AllProduct = soup.find_all('li', class_='s-item s-item__pl-on-bottom')\n",
    "    for data in AllProduct:\n",
    "        if AllProduct.index(data) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            productName  =  data.find('div', class_='s-item__title').text\n",
    "        except:\n",
    "            continue\n",
    "        try :    \n",
    "            productSubtitle = data.find('div', class_='s-item__subtitle').text\n",
    "        except:\n",
    "            productSubtitle = ''\n",
    "        try:\n",
    "            productRatingCount  = data.find('div', class_='x-star-rating').text\n",
    "        except :\n",
    "            productRatingCount = '0'\n",
    "        try:\n",
    "            ProductTotalRating = data.find('span', class_='s-item__reviews-count').text\n",
    "        except:\n",
    "            ProductTotalRating = '0'\n",
    "        productLink = data.find('a', class_='s-item__link').get('href')\n",
    "        ProductTotalRating = re.findall(r'^\\d+', ProductTotalRating)[0]\n",
    "        ProductPrice = data.find('span', class_='s-item__price').text\n",
    "        productshiping=data.find('span',class_='s-item__itemLocation')\n",
    "        if productshiping:\n",
    "            productshiping=productshiping.text.strip()\n",
    "        else:\n",
    "            productshiping='location not available'\n",
    "        productseller=data.find('span',class_='s-item__seller-info')\n",
    "        if productseller:\n",
    "            productseller=productseller.text.strip()\n",
    "        else:\n",
    "            productseller='seller not available'\n",
    "\n",
    "        if ProductPrice:\n",
    "            price = int(re.findall(r'\\d+', ProductPrice)[0])\n",
    "        if price> 100 and productRatingCount >= 100 :\n",
    "            \n",
    "            prouducts = {'productName': productName, 'productSubtitle': productSubtitle ,'ProductPrice': ProductPrice  , 'productRatingCount': productRatingCount, 'ProductTotalRating': ProductTotalRating , 'productshiping':productshiping,'productseller':productseller, 'productLink': productLink}\n",
    "            ItemList.append(prouducts)\n",
    "    return ItemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "totalPages =  1000\n",
    "target_brands = [\" Rockshox\", \"Shimano\", \"raceface\"]\n",
    "querry = \"mountain biking\"\n",
    "TotalData = []\n",
    "for brand in target_brands:\n",
    "    page =1\n",
    "    querry = querry + brand\n",
    "    while page < totalPages :\n",
    "        data = getpageData(querry, page)\n",
    "        print(page)\n",
    "        TotalData.extend(data)\n",
    "        df = pd.DataFrame(TotalData)\n",
    "        df.to_excel('ebay.xlsx', index=False)\n",
    "        page+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ebay.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_page_html(query, page):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        '_from': 'R40',\n",
    "        '_nkw': query,\n",
    "        '_sop': '12',  # Sort by Best Match\n",
    "        '_ipg': '100',  # Items per page\n",
    "        '_pgn': page\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://www.ebay.com/sch/i.html', params=params, headers=headers)\n",
    "    return response.text\n",
    "\n",
    "def extract_data_from_page(html, target_brands):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    all_products = soup.find_all('div', class_='s-item__info')\n",
    "\n",
    "    data = []\n",
    "    for product in all_products:\n",
    "        brand_tags = product.find_all('span', class_='s-item__dynamic')\n",
    "        brands = [tag.text.strip() for tag in brand_tags]\n",
    "\n",
    "        # Check if the product has any of the desired brands\n",
    "        if any(brand in brands for brand in target_brands):\n",
    "            seller_name = product.find('span', class_='s-item__seller-info').text.strip()\n",
    "            rating_count = int(re.search(r'\\d+', product.find('span', class_='s-item__reviews-count').text.strip()).group())\n",
    "            price = float(re.search(r'\\d+\\.\\d+', product.find('span', class_='s-item__price').text.strip()).group())\n",
    "\n",
    "            # Filter sellers with several hundred ratings and average item price over $100\n",
    "            if rating_count >= 200 and price > 100:\n",
    "                data.append({'SellerName': seller_name, 'RatingCount': rating_count, 'Price': price, 'Brands': brands})\n",
    "\n",
    "    return data\n",
    "\n",
    "def scrape_ebay(query, total_pages, target_brands):\n",
    "    all_data = []\n",
    "    for page in range(1, total_pages + 1):\n",
    "        html = get_page_html(query, page)\n",
    "        page_data = extract_data_from_page(html, target_brands)\n",
    "        all_data.extend(page_data)\n",
    "        print(f\"Scraped page {page}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = \"mountain bike\"\n",
    "   \n",
    "    total_pages_to_scrape = int(input(\"Enter the total number of pages to scrape: \"))\n",
    "    scraped_data = scrape_ebay(search_query, total_pages_to_scrape, target_brands)\n",
    "    #Convert data to DataFrame\n",
    "    dataframe = pd.DataFrame(scraped_data)\n",
    "    # Save data to Excel\n",
    "    dataframe.to_excel(f\"{search_query}_ebay_data.xlsx\", index=False)\n",
    "    print(\"Scraping completed. Data saved to Excel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "querry = \"mountain biking\"\n",
    "page = 1\n",
    "response = getHtml(querry, page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ebay.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_page_link = soup.find_all('a', class_='pagination__item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in last_page_link:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
